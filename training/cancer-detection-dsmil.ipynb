{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":45867,"databundleVersionId":6924515,"sourceType":"competition"},{"sourceId":6774553,"sourceType":"datasetVersion","datasetId":3898019},{"sourceId":6917177,"sourceType":"datasetVersion","datasetId":3889865}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cancer Detection With Tiles\n\nTry detection with tiles, where for each large image, we randomly select a tile and assign it the original image's label. \n\nWe can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \nWe can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-11-02T16:11:09.318503Z","iopub.execute_input":"2023-11-02T16:11:09.318811Z","iopub.status.idle":"2023-11-02T16:11:09.327423Z","shell.execute_reply.started":"2023-11-02T16:11:09.318777Z","shell.execute_reply":"2023-11-02T16:11:09.326036Z"}}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import pytorch_lightning as pl\nimport torch\nfrom torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights, convnext_base, ConvNeXt_Base_Weights, convnext_small, ConvNeXt_Small_Weights, efficientnet_b4, EfficientNet_B4_Weights\nimport torch.nn as nn\nimport torch.optim as optim\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\nfrom typing import List, Dict, Optional\nimport pandas as pd\nimport numpy as np\nimport os\n\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nimport random\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\nimport random\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2023-12-06T17:58:21.068799Z","iopub.execute_input":"2023-12-06T17:58:21.069522Z","iopub.status.idle":"2023-12-06T17:58:37.489223Z","shell.execute_reply.started":"2023-12-06T17:58:21.069483Z","shell.execute_reply":"2023-12-06T17:58:37.488144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom glob import glob\nfrom openslide import ImageSlide, open_slide\nfrom pprint import pprint\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:01:23.700865Z","iopub.execute_input":"2023-12-06T18:01:23.701310Z","iopub.status.idle":"2023-12-06T18:01:23.706824Z","shell.execute_reply.started":"2023-12-06T18:01:23.701277Z","shell.execute_reply":"2023-12-06T18:01:23.705808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# installing pyvips for offline use\n\n!ls /kaggle/input/pyvips-python-and-deb-package-gpu\n# intall the deb packages\n!yes | dpkg -i --force-depends /kaggle/input/pyvips-python-and-deb-package-gpu/linux_packages/archives/*.deb\n# install the python wrapper\n!pip install pyvips -f /kaggle/input/pyvips-python-and-deb-package-gpu/python_packages/ --no-index","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"scrolled":true,"execution":{"iopub.status.busy":"2023-12-06T17:58:37.582730Z","iopub.execute_input":"2023-12-06T17:58:37.583124Z","iopub.status.idle":"2023-12-06T17:59:38.746938Z","shell.execute_reply.started":"2023-12-06T17:58:37.583086Z","shell.execute_reply":"2023-12-06T17:59:38.745650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pyvips settings - important\nos.environ['VIPS_CONCURRENCY'] = '4'\nos.environ['VIPS_DISC_THRESHOLD'] = '15gb'\n\nimport PIL\nPIL.Image.MAX_IMAGE_PIXELS = 933120000","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:06:00.819133Z","iopub.execute_input":"2023-12-06T18:06:00.819898Z","iopub.status.idle":"2023-12-06T18:06:00.824735Z","shell.execute_reply.started":"2023-12-06T18:06:00.819861Z","shell.execute_reply":"2023-12-06T18:06:00.823725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyvips\n\ntrain_images = glob(\"/kaggle/input/UBC-OCEAN/train_images/*\")\nimg_path = train_images[2]\nprint(img_path)\nimage = pyvips.Image.new_from_file(img_path)\nimage.tiffsave(\"/kaggle/working/image.tif\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:06:04.128134Z","iopub.execute_input":"2023-12-06T18:06:04.128881Z","iopub.status.idle":"2023-12-06T18:06:27.415348Z","shell.execute_reply.started":"2023-12-06T18:06:04.128846Z","shell.execute_reply":"2023-12-06T18:06:27.414224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slide = ImageSlide(\"/kaggle/working/image.tif\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:07:09.394602Z","iopub.execute_input":"2023-12-06T18:07:09.395214Z","iopub.status.idle":"2023-12-06T18:07:09.401355Z","shell.execute_reply.started":"2023-12-06T18:07:09.395181Z","shell.execute_reply":"2023-12-06T18:07:09.400491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone \"https://github.com/binli123/dsmil-wsi.git\"","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:23:00.241589Z","iopub.execute_input":"2023-12-06T18:23:00.242399Z","iopub.status.idle":"2023-12-06T18:23:05.146505Z","shell.execute_reply.started":"2023-12-06T18:23:00.242366Z","shell.execute_reply":"2023-12-06T18:23:05.145273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/NVIDIA/apex","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/apex\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:40:28.246046Z","iopub.execute_input":"2023-12-06T18:40:28.247076Z","iopub.status.idle":"2023-12-06T18:40:28.251718Z","shell.execute_reply.started":"2023-12-06T18:40:28.247037Z","shell.execute_reply":"2023-12-06T18:40:28.250763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if pip >= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key... \n# pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./\n# otherwise\n!pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-06T18:40:36.291358Z","iopub.execute_input":"2023-12-06T18:40:36.292054Z","iopub.status.idle":"2023-12-06T18:40:56.834251Z","shell.execute_reply.started":"2023-12-06T18:40:36.292015Z","shell.execute_reply":"2023-12-06T18:40:56.833025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/dsmil-wsi/simclr\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:42:27.848208Z","iopub.execute_input":"2023-12-06T18:42:27.849384Z","iopub.status.idle":"2023-12-06T18:42:27.854519Z","shell.execute_reply.started":"2023-12-06T18:42:27.849344Z","shell.execute_reply":"2023-12-06T18:42:27.853448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"config.yaml\", \"w\") as f:\n    f.write(\"\"\"batch_size: 64\nepochs: 100\neval_every_n_epochs: 1\nfine_tune_from: ''\nlog_every_n_steps: 25\nweight_decay: 10e-6\nfp16_precision: False\nn_gpu: 1\ngpu_ids: (0,)\n\nmodel:\n  out_dim: 256\n  base_model: \"resnet18\"\n\ndataset:\n  s: 1\n  input_shape: (224,224,3)\n  num_workers: 0\n  valid_size: 0.1\n\nloss:\n  temperature: 0.5\n  use_cosine_similarity: True\n\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"run.py\", \"w\") as f:\n    f.write(\"\"\"\nfrom simclr import SimCLR\nimport yaml\nfrom data_aug.dataset_wrapper import DataSetWrapper\nimport os, glob\nimport pandas as pd\nimport argparse\n\ndef generate_csv(args):\n    if args.multiscale==0:\n        path_temp = os.path.join('..', '/kaggle/input/tiles-of-cancer-2048px-scale-0-25', '*', '*.png')\n        patch_path = glob.glob(path_temp) # /class_name/bag_name/*.jpeg\n    df = pd.DataFrame(patch_path)\n    df.to_csv('all_patches.csv', index=False)\n        \n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--level', type=str, default='low', help='Magnification level to compute embedder (low/high)')\n    parser.add_argument('--multiscale', type=int, default=0, help='Whether the patches are cropped from multiscale (0/1-no/yes)')\n    parser.add_argument('--dataset', type=str, default='TCGA-lung', help='Dataset folder name')\n    args = parser.parse_args()\n    config = yaml.load(open(\"config.yaml\", \"r\"), Loader=yaml.FullLoader)\n    gpu_ids = eval(config['gpu_ids'])\n    os.environ['CUDA_VISIBLE_DEVICES']=','.join(str(x) for x in gpu_ids)   \n    dataset = DataSetWrapper(config['batch_size'], **config['dataset'])   \n    generate_csv(args)\n    simclr = SimCLR(dataset, config)\n    simclr.train()\n\n\nif __name__ == \"__main__\":\n    main()\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2023-12-06T19:22:07.540000Z","iopub.execute_input":"2023-12-06T19:22:07.540461Z","iopub.status.idle":"2023-12-06T19:22:07.547088Z","shell.execute_reply.started":"2023-12-06T19:22:07.540423Z","shell.execute_reply":"2023-12-06T19:22:07.546026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python run.py --dataset=\"UBC-OCEAN\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Classes","metadata":{}},{"cell_type":"code","source":"class AugmentationTransforms:\n    def __init__(self, image_size: int):\n        self.image_size = image_size\n\n    def get_training_augmentation(self):\n        train_transform = [\n            albu.HorizontalFlip(p=0.5),\n            albu.VerticalFlip(p=0.5),\n            albu.augmentations.transforms.GaussNoise(p=0.2),\n            albu.OneOf(\n                [\n                    albu.CLAHE(p=1),\n                    albu.RandomBrightnessContrast(p=1),\n                    albu.RandomGamma(p=1),\n                    albu.HueSaturationValue(p=1),\n                ],\n                p=0.5,\n            ),\n#             albu.OneOf(\n#                 [\n#                     albu.augmentations.transforms.Sharpen(p=1),\n#                     albu.Blur(blur_limit=3, p=1),\n#                     albu.MotionBlur(blur_limit=3, p=1),\n#                 ],\n#                 p=0.5,\n#             ),\n              albu.augmentations.geometric.resize.Resize(\n                self.image_size, self.image_size, always_apply=True\n            ),\n        ]\n        return albu.Compose(train_transform)\n\n    def get_validation_augmentation(self):\n        \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n        test_transform = [\n            albu.augmentations.geometric.resize.Resize(\n                self.image_size, self.image_size, always_apply=True\n            ),\n        ]\n        return albu.Compose(test_transform)\n\n    def get_preprocessing(self):\n        \"\"\"Construct preprocessing transform\n\n        Args:\n            preprocessing_fn (callbale): data normalization function\n                (can be specific for each pretrained neural network)\n        Return:\n            transform: albumentations.Compose\n\n        \"\"\"\n\n        # Model expects input [N, C, H, W]\n        # ToTensor convert HWC image to CHW image\n        transform = [\n            albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2(),\n        ]\n\n        return albu.Compose(transform)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T16:41:45.283282Z","iopub.execute_input":"2023-11-12T16:41:45.28438Z","iopub.status.idle":"2023-11-12T16:41:45.294269Z","shell.execute_reply.started":"2023-11-12T16:41:45.284346Z","shell.execute_reply":"2023-11-12T16:41:45.293262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nimport pandas as pd\nimport cv2\n\n\nclass CancerDataset(Dataset):\n    def __init__(\n        self,\n        data_df: pd.DataFrame,\n        black_threshold: int = 20,\n        max_bg_threshold: float = 0.35,\n        preprocessing=None,\n        augmentation=None,\n    ):\n        self.data_df = data_df\n        self.black_threshold = black_threshold\n        self.max_bg_threshold = max_bg_threshold\n        self.preprocessing = preprocessing\n        self.augmentation = augmentation\n\n        self.classes = [\"HGSC\", \"LGSC\", \"EC\", \"CC\", \"MC\"]\n        self.class2idx = {label: idx for idx, label in enumerate(self.classes)}\n\n        self.data_df.loc[:, \"label\"] = self.data_df.loc[:, \"label\"].map(self.class2idx)\n        self.images_lists = [\n            list(\n                Path(\n                    \"/kaggle/input/tiles-of-cancer-2048px-scale-0-25/\", str(image_id)\n                ).rglob(\"**/*.png\")\n            )\n            for image_id in self.data_df[\"image_id\"]\n        ]\n\n    def __getitem__(self, i):\n        image_id, label, _, _, _ = self.data_df.iloc[i]\n\n        image_id_list = self.images_lists[i]\n        # or\n        # image_list = list(Path(\"/kaggle/input/tiles-of-cancer-2048px-scale-0-25/\", str(image_id)).rglob(\"**/*.png\"))\n        random.shuffle(image_id_list)\n\n        for img_path in image_id_list:\n            img = cv2.imread(str(img_path))\n\n            black_bg = np.sum(img, axis=2) <= self.black_threshold\n            if np.sum(black_bg) <= (np.prod(black_bg.shape) * self.max_bg_threshold):\n                break\n\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        if self.augmentation:\n            img = self.augmentation(image=img)[\"image\"]\n\n        if self.preprocessing:\n            img = self.preprocessing(image=img)[\"image\"]\n\n        return img, label\n\n    def __len__(self):\n        return len(self.data_df)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T16:41:48.473303Z","iopub.execute_input":"2023-11-12T16:41:48.474056Z","iopub.status.idle":"2023-11-12T16:41:48.486027Z","shell.execute_reply.started":"2023-11-12T16:41:48.474023Z","shell.execute_reply":"2023-11-12T16:41:48.484992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CancerDataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        image_size: int,\n        batch_size: int,\n        cutoff: float = 0.8,\n        black_threshold: int = 20,\n        max_bg_threshold: float = 0.35,\n        shuffle: Optional[bool] = True,\n    ):\n        super().__init__()\n        self.image_size = image_size\n        self.train_batch_size = batch_size\n        self.shuffle = shuffle\n        self.cutoff = cutoff\n        self.black_threshold = black_threshold\n        self.max_bg_threshold = max_bg_threshold\n\n        aug_transforms = AugmentationTransforms(self.image_size)\n\n        self.preprocess_transforms = aug_transforms.get_preprocessing()\n        self.train_transforms = aug_transforms.get_training_augmentation()\n        self.val_transforms = aug_transforms.get_validation_augmentation()\n\n    def setup(self, stage: Optional[str] = None):\n        if stage == \"fit\":\n            train_csv = pd.read_csv(\n                \"/kaggle/input/tiles-of-cancer-2048px-scale-0-25/train.csv\"\n            )\n            cutoff_point = int(len(train_csv) * self.cutoff)\n\n            self.train_dataset = CancerDataset(\n                train_csv.iloc[:cutoff_point],\n                self.black_threshold,\n                self.max_bg_threshold,\n                self.preprocess_transforms,\n                self.train_transforms,\n            )\n            self.validation_dataset = CancerDataset(\n                train_csv.iloc[cutoff_point:],\n                self.black_threshold,\n                self.max_bg_threshold,\n                self.preprocess_transforms,\n                self.val_transforms,\n            )\n\n            print(f\"The the training set has {len(self.train_dataset)} images\")\n            print(f\"The the validation set has {len(self.validation_dataset)} images\")\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.train_batch_size,\n            shuffle=self.shuffle,\n            num_workers=4,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.validation_dataset,\n            batch_size=8,\n            shuffle=False,\n            num_workers=4,\n        )\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T16:41:50.273886Z","iopub.execute_input":"2023-11-12T16:41:50.274621Z","iopub.status.idle":"2023-11-12T16:41:50.285623Z","shell.execute_reply.started":"2023-11-12T16:41:50.274589Z","shell.execute_reply":"2023-11-12T16:41:50.284633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchmetrics\n\n\nclass CancerDetector(pl.LightningModule):\n    def __init__(\n        self,\n        lr: float,\n        gamma: float,\n        model_name: str,\n        warmup_epochs: int = 4,\n        num_classes: int = 5,\n        init_weights: bool = True,\n    ):\n        super().__init__()\n        # TODO Use model preprocessing function\n        self.model = self._get_model(model_name, num_classes, init_weights)\n\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.lr = lr\n        self.gamma = gamma\n        self.warmup_epochs = warmup_epochs\n\n        self.save_hyperparameters()\n\n        # Should we use macro average? Default is micro\n        self.accuracy = torchmetrics.classification.Accuracy(\n            num_classes=num_classes, task=\"multiclass\"\n        )\n        self.f1 = torchmetrics.classification.F1Score(\n            num_classes=num_classes, task=\"multiclass\"\n        )\n        self.recall = torchmetrics.classification.Recall(\n            num_classes=num_classes, task=\"multiclass\"\n        )\n        self.precision = torchmetrics.classification.Precision(\n            num_classes=num_classes, task=\"multiclass\"\n        )\n\n    def _get_model(self, model_name: str, num_classes: int, init_weights: bool):\n        if model_name == \"convnext_tiny\":\n            model = convnext_tiny(\n                weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1 if init_weights else None\n            )\n        elif model_name == \"convnext_small\":\n            model = convnext_small(\n                weights=ConvNeXt_Small_Weights.IMAGENET1K_V1 if init_weights else None\n            )\n        elif model_name == \"convnext_base\":\n            model = convnext_base(\n                weights=ConvNeXt_Base_Weights.IMAGENET1K_V1 if init_weights else None\n            )\n        elif model_name == \"efficientnet_b4\":\n            model = efficientnet_b4(weights = EfficientNet_B4_Weights.IMAGENET1K_V1)\n        else:\n            raise Exception(f\"Unknown model name {model_name}\")\n        \n        if \"covnext\" in model_name:\n            in_features = model.classifier[2].in_features\n            model.classifier[2] = nn.Linear(in_features, num_classes)\n        else:\n            in_features = model.classifier[1].in_features\n            model.classifier[1] = nn.Linear(in_features, num_classes)\n\n        return model\n\n    def forward(self, imgs: torch.Tensor):\n        return self.model(imgs)\n\n    def training_step(self, batch: torch.Tensor, batch_idx: int):\n        x, y = batch\n        output = self(x)\n        loss = self.loss_fn(output, y)\n\n        self._log_metrics(loss, output, y, \"train\")\n\n        return loss\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int):\n        x, y = batch\n        output = self(x)\n        loss = self.loss_fn(output, y)\n\n        self._log_metrics(loss, output, y, \"val\")\n\n        return loss\n\n    def _log_metrics(\n        self, loss: torch.Tensor, preds: torch.Tensor, target: torch.Tensor, phase: str\n    ):\n        accuracy = self.accuracy(preds, target)\n        f1 = self.f1(preds, target)\n        recall = self.recall(preds, target)\n        precision = self.precision(preds, target)\n\n        self.log(f\"{phase}/loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n        self.log(\n            f\"{phase}/accuracy\", accuracy, prog_bar=True, on_step=False, on_epoch=True\n        )\n        self.log(f\"{phase}/f1\", f1, prog_bar=True, on_step=False, on_epoch=True)\n        self.log(f\"{phase}/recall\", recall, prog_bar=True, on_step=False, on_epoch=True)\n        self.log(\n            f\"{phase}/precision\", precision, prog_bar=True, on_step=False, on_epoch=True\n        )\n\n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.model.parameters(), lr=self.lr)\n        \n        warmup = optim.lr_scheduler.LinearLR(optimizer, total_iters=self.warmup_epochs)\n        exponential = optim.lr_scheduler.ExponentialLR(optimizer, gamma=self.gamma)\n        scheduler = optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup, exponential], milestones=[self.warmup_epochs])\n        \n        return [optimizer], [scheduler]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-12T16:41:50.684101Z","iopub.execute_input":"2023-11-12T16:41:50.684485Z","iopub.status.idle":"2023-11-12T16:41:50.705896Z","shell.execute_reply.started":"2023-11-12T16:41:50.684455Z","shell.execute_reply":"2023-11-12T16:41:50.704713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization\nVisualize inputs before the network","metadata":{}},{"cell_type":"code","source":"# Visualize input images\ndef visualize_input(datamodule: pl.LightningDataModule):\n    mean=np.array((0.485, 0.456, 0.406)) * 255\n    std=np.array((0.229, 0.224, 0.225)) * 255\n    \n    datamodule.prepare_data()\n    datamodule.setup(\"fit\")\n    train = datamodule.train_dataloader()\n    imgs, labels = next(iter(train))\n    \n    # B x C x H x W to B x H x W x C\n    imgs = imgs.permute((0,2,3,1))\n    imgs = imgs * std + mean\n    # Change the order of channels\n    imgs = imgs.flip(3)\n    imgs = imgs.numpy().astype('uint8')\n    \n    classes = ['HGSC', 'LGSC', 'EC', 'CC', 'MC']\n    idx2class = {idx: class_name for idx, class_name in enumerate(classes)}\n    \n    plt.rcParams['figure.figsize'] = (8.0, 8.0) # set default size of plots\n\n    for i, (img, label) in enumerate(zip(imgs, labels)):\n        plt.subplot(4, 4, 1 + i)\n        plt.imshow(img)\n        plt.title(idx2class[label.item()])\n        plt.axis('off')\n        \n        if i == 15:\n            break\n            \n    plt.show()\n    \ncancer_module = CancerDataModule(320, 16)\nvisualize_input(cancer_module)","metadata":{"execution":{"iopub.status.busy":"2023-11-12T16:41:51.662839Z","iopub.execute_input":"2023-11-12T16:41:51.663819Z","iopub.status.idle":"2023-11-12T16:42:30.043414Z","shell.execute_reply.started":"2023-11-12T16:41:51.663785Z","shell.execute_reply":"2023-11-12T16:42:30.042444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"!echo -e \"machine api.wandb.ai\\n  login user\\n  password 33b28a5d3e362bc21dfe1fc1759af32fdd74dec0\" >> /root/.netrc","metadata":{"execution":{"iopub.status.busy":"2023-11-11T13:00:50.902477Z","iopub.execute_input":"2023-11-11T13:00:50.902856Z","iopub.status.idle":"2023-11-11T13:00:51.884451Z","shell.execute_reply.started":"2023-11-11T13:00:50.902826Z","shell.execute_reply":"2023-11-11T13:00:51.883406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    \"lr\": 3e-3,\n    \"lr_decay_gamma\": 0.98,\n    \"warmup_epochs\": 4,\n    \"model_name\": \"efficientnet_b4\",\n    \"input_size\": 512,\n    \"batch_size\": 16,\n    \"epochs\": 200,\n    \"train_val_cutoff\": 0.8,\n    \"black_threshold\": 20,\n    \"max_bg_threshold\": 0.3,\n}\n\npl.seed_everything(seed=31415, workers=True)\n\nwandb_logger = WandbLogger(project=\"UBC Ovarian Cancer Classification\", log_model=False)\nmodel = CancerDetector(\n    lr=config[\"lr\"],\n    gamma=config[\"lr_decay_gamma\"],\n    model_name=config[\"model_name\"],\n    warmup_epochs=config[\"warmup_epochs\"],\n)\ncancer_module = CancerDataModule(\n    config[\"input_size\"],\n    config[\"batch_size\"],\n    cutoff=config[\"train_val_cutoff\"],\n    black_threshold=config[\"black_threshold\"],\n    max_bg_threshold=config[\"max_bg_threshold\"],\n)\n\n# Initialize callbacks\nlr_monitor = LearningRateMonitor()\n# early_stopping = EarlyStopping(\n#     monitor=\"val/f1\", min_delta=0.0002, patience=8, mode=\"max\"\n# )\ncheckpoints = ModelCheckpoint(\n    monitor=\"val/f1\",\n    save_top_k=3,\n    mode=\"max\",\n    save_weights_only=True,\n    save_last=True,\n    auto_insert_metric_name=False,\n    filename=\"epoch={epoch}-loss={val/loss:.4f}-f1={val/f1:.4f}\",\n)\n\ntrainer = pl.Trainer(\n    logger=wandb_logger,\n    max_epochs=config[\"epochs\"],\n    accelerator=\"gpu\",\n    devices=1,\n    precision=\"16-mixed\",\n    callbacks=[lr_monitor, checkpoints], #early_stopping,\n)\n\ntrainer.fit(model, datamodule=cancer_module)\nprint(trainer.checkpoint_callback.best_model_path)\n\nwith open(\"best_model.txt\", \"w\") as f:\n    f.write(trainer.checkpoint_callback.best_model_path)\n\n\ntrainer.save_checkpoint(\"cancer_classification_model.pt\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-11T13:00:51.885981Z","iopub.execute_input":"2023-11-11T13:00:51.886293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TODO\n#### Validate Code\n- Fix seed\n- [x] Overfit a batch with a single image\n- [x] Input independent baseline test\n#### Fit\n- [x] Bigger model\n- [x] weight decay\n- [ ] augmentations\n- [ ] early stopping\n- [ ] lr scheduler\n- [ ] hyper parameters","metadata":{"execution":{"iopub.status.busy":"2023-10-27T15:15:27.467057Z","iopub.execute_input":"2023-10-27T15:15:27.467336Z","iopub.status.idle":"2023-10-27T15:15:28.419678Z","shell.execute_reply.started":"2023-10-27T15:15:27.46731Z","shell.execute_reply":"2023-10-27T15:15:28.418476Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}