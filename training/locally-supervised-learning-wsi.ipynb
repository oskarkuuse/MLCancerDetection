{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":45867,"databundleVersionId":6924515,"sourceType":"competition"},{"sourceId":7048067,"sourceType":"datasetVersion","datasetId":4055832},{"sourceId":6774553,"sourceType":"datasetVersion","datasetId":3898019}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# installing pyvips for offline use\n# intall the deb packages\n!yes | dpkg -i --force-depends /kaggle/input/pyvips-python-and-deb-package-gpu/linux_packages/archives/*.deb &> /dev/null\n# install the python wrapper\n!pip install pyvips -f /kaggle/input/pyvips-python-and-deb-package-gpu/python_packages/ --no-index &> /dev/null","metadata":{"execution":{"iopub.status.busy":"2023-11-25T11:47:46.857394Z","iopub.execute_input":"2023-11-25T11:47:46.857771Z","iopub.status.idle":"2023-11-25T11:48:47.841672Z","shell.execute_reply.started":"2023-11-25T11:47:46.857743Z","shell.execute_reply":"2023-11-25T11:48:47.840263Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"yes: standard output: Broken pipe\n","output_type":"stream"}]},{"cell_type":"code","source":"import pyvips\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path","metadata":{"execution":{"iopub.status.busy":"2023-11-25T11:48:47.844819Z","iopub.execute_input":"2023-11-25T11:48:47.845283Z","iopub.status.idle":"2023-11-25T11:48:48.443629Z","shell.execute_reply.started":"2023-11-25T11:48:47.845242Z","shell.execute_reply":"2023-11-25T11:48:48.442875Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/UBC-OCEAN/train.csv\", usecols=[\"label\", \"image_id\", \"is_tma\"])\n\nwsi_dir = Path(\"/kaggle/input/UBC-OCEAN/train_thumbnails\")\ntma_dir = Path(\"/kaggle/input/UBC-OCEAN/train_images\")\n\ndf[\"image_id\"] = [tma_dir / f\"{row['image_id']}.png\" if row[\"is_tma\"] else wsi_dir / f\"{row['image_id']}_thumbnail.png\" for _, row in df.iterrows()]\n\nclasses = ['HGSC', 'LGSC', 'EC', 'CC', 'MC']\nclass2idx = {label: idx for idx, label in enumerate(classes)}\n\ndf[\"label\"] = df[\"label\"].map(class2idx)\ndf[\"type\"] = [\"train\"] * len(df)\ndf.rename(columns={\"image_id\": \"slide_id\"}, inplace=True)\n\n# In-place editing\ndf_test = df.sample(frac=0.2)\ndf.loc[df_test.index, \"type\"] = [\"valid\"] * len(df_test)\n\ndf = df.drop(columns=[\"is_tma\"])\n\ndf.to_csv(\"wsi_train.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T11:49:40.609704Z","iopub.execute_input":"2023-11-25T11:49:40.610596Z","iopub.status.idle":"2023-11-25T11:49:40.696513Z","shell.execute_reply.started":"2023-11-25T11:49:40.610560Z","shell.execute_reply":"2023-11-25T11:49:40.695543Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/UBC-OCEAN/train.csv\")\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-25T11:49:41.796050Z","iopub.execute_input":"2023-11-25T11:49:41.796462Z","iopub.status.idle":"2023-11-25T11:49:41.821860Z","shell.execute_reply.started":"2023-11-25T11:49:41.796433Z","shell.execute_reply":"2023-11-25T11:49:41.820904Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"           image_id    image_width  image_height\ncount    538.000000     538.000000    538.000000\nmean   32194.340149   48859.533457  29729.460967\nstd    18774.950592   20040.989927  10762.899796\nmin        4.000000    2964.000000   2964.000000\n25%    15881.250000   34509.000000  22089.500000\n50%    32152.000000   48160.000000  29732.000000\n75%    47892.500000   64143.750000  37880.750000\nmax    65533.000000  105763.000000  50155.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>image_width</th>\n      <th>image_height</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>538.000000</td>\n      <td>538.000000</td>\n      <td>538.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>32194.340149</td>\n      <td>48859.533457</td>\n      <td>29729.460967</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>18774.950592</td>\n      <td>20040.989927</td>\n      <td>10762.899796</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4.000000</td>\n      <td>2964.000000</td>\n      <td>2964.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>15881.250000</td>\n      <td>34509.000000</td>\n      <td>22089.500000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>32152.000000</td>\n      <td>48160.000000</td>\n      <td>29732.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>47892.500000</td>\n      <td>64143.750000</td>\n      <td>37880.750000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>65533.000000</td>\n      <td>105763.000000</td>\n      <td>50155.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!head wsi_train.csv","metadata":{"execution":{"iopub.status.busy":"2023-11-25T11:49:42.089122Z","iopub.execute_input":"2023-11-25T11:49:42.089490Z","iopub.status.idle":"2023-11-25T11:49:43.034668Z","shell.execute_reply.started":"2023-11-25T11:49:42.089461Z","shell.execute_reply":"2023-11-25T11:49:43.033504Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"slide_id,label,type\n/kaggle/input/UBC-OCEAN/train_thumbnails/4_thumbnail.png,0,train\n/kaggle/input/UBC-OCEAN/train_thumbnails/66_thumbnail.png,1,train\n/kaggle/input/UBC-OCEAN/train_images/91.png,0,train\n/kaggle/input/UBC-OCEAN/train_thumbnails/281_thumbnail.png,1,train\n/kaggle/input/UBC-OCEAN/train_thumbnails/286_thumbnail.png,2,valid\n/kaggle/input/UBC-OCEAN/train_thumbnails/431_thumbnail.png,0,train\n/kaggle/input/UBC-OCEAN/train_thumbnails/706_thumbnail.png,0,train\n/kaggle/input/UBC-OCEAN/train_thumbnails/970_thumbnail.png,0,train\n/kaggle/input/UBC-OCEAN/train_thumbnails/1020_thumbnail.png,0,train\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir output","metadata":{"execution":{"iopub.status.busy":"2023-11-25T11:49:43.601434Z","iopub.execute_input":"2023-11-25T11:49:43.602327Z","iopub.status.idle":"2023-11-25T11:49:44.591319Z","shell.execute_reply.started":"2023-11-25T11:49:43.602292Z","shell.execute_reply":"2023-11-25T11:49:44.589890Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!rm local_learning_wsi -rf\n!git clone https://github.com/karl-joan/local_learning_wsi.git","metadata":{"execution":{"iopub.status.busy":"2023-11-25T11:49:44.593519Z","iopub.execute_input":"2023-11-25T11:49:44.593896Z","iopub.status.idle":"2023-11-25T11:49:47.122325Z","shell.execute_reply.started":"2023-11-25T11:49:44.593860Z","shell.execute_reply":"2023-11-25T11:49:47.121206Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Cloning into 'local_learning_wsi'...\nremote: Enumerating objects: 100, done.\u001b[K\nremote: Counting objects: 100% (100/100), done.\u001b[K\nremote: Compressing objects: 100% (71/71), done.\u001b[K\nremote: Total 100 (delta 54), reused 63 (delta 28), pack-reused 0\u001b[K\nReceiving objects: 100% (100/100), 442.50 KiB | 12.29 MiB/s, done.\nResolving deltas: 100% (54/54), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!echo -e \"machine api.wandb.ai\\n  login user\\n  password 33b28a5d3e362bc21dfe1fc1759af32fdd74dec0\" >> /root/.netrc","metadata":{"execution":{"iopub.status.busy":"2023-11-25T11:49:47.467206Z","iopub.execute_input":"2023-11-25T11:49:47.467585Z","iopub.status.idle":"2023-11-25T11:49:48.408242Z","shell.execute_reply.started":"2023-11-25T11:49:47.467555Z","shell.execute_reply":"2023-11-25T11:49:48.407168Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!python local_learning_wsi/main.py /kaggle/input/UBC-OCEAN/train_thumbnails \\\n    /kaggle/working/wsi_train.csv \\\n    --num-workers 1 \\\n    --num-classes 5 \\\n    --output-dir /kaggle/working/output \\\n    --precision 16-mixed \\\n    --gpu-id 0 \\\n    --batch-size 1 \\\n    --project-name \"UBC Ovarian Cancer Classification\" \\\n    --epochs 5 ","metadata":{"execution":{"iopub.status.busy":"2023-11-25T11:49:48.410198Z","iopub.execute_input":"2023-11-25T11:49:48.410538Z","iopub.status.idle":"2023-11-25T11:54:28.339655Z","shell.execute_reply.started":"2023-11-25T11:49:48.410510Z","shell.execute_reply":"2023-11-25T11:54:28.338629Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83.3M/83.3M [00:00<00:00, 239MB/s]\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarl-joan-alesma\u001b[0m (\u001b[33mmalev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.0\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20231125_115003-q7mgn5aa\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtest_run\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/malev/UBC%20Ovarian%20Cancer%20Classification\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/malev/UBC%20Ovarian%20Cancer%20Classification/runs/q7mgn5aa\u001b[0m\nAdjusting learning rate of group 0 to 1.0000e-05.\nAdjusting learning rate of group 1 to 2.0000e-05.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\nEpoch 0:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 125/430 [01:01<02:29,  2.04it/s, v_num=n5aa]====================\nfetures_large.shape: torch.Size([1, 64, 108, 397])\nimage_ori_large.shape: torch.Size([1, 64, 108, 397])\npatch_size: 128\nLarge space: (-20, 269)\nOri space: (-20, 269)\nSampling space: (-20, 269)\nRuntime Error random_ expects 'from' to be less than 'to', but got from=0 >= to=-5380\nRun Again......1/2\nHalfing image size\n====================\nfetures_large.shape: torch.Size([1, 64, 72, 265])\nimage_ori_large.shape: torch.Size([1, 64, 72, 265])\npatch_size: 128\nLarge space: (-56, 137)\nOri space: (-56, 137)\nSampling space: (-56, 137)\nRuntime Error random_ expects 'from' to be less than 'to', but got from=0 >= to=-7672\nRun Again......2/2\nGive up!\nEpoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 267/430 [01:59<01:12,  2.24it/s, v_num=n5aa]====================\nfetures_large.shape: torch.Size([1, 64, 89, 500])\nimage_ori_large.shape: torch.Size([1, 64, 89, 500])\npatch_size: 128\nLarge space: (-39, 372)\nOri space: (-39, 372)\nSampling space: (-39, 372)\nRuntime Error random_ expects 'from' to be less than 'to', but got from=0 >= to=-14508\nRun Again......1/2\nHalfing image size\n====================\nfetures_large.shape: torch.Size([1, 64, 59, 334])\nimage_ori_large.shape: torch.Size([1, 64, 59, 334])\npatch_size: 128\nLarge space: (-69, 206)\nOri space: (-69, 206)\nSampling space: (-69, 206)\nRuntime Error random_ expects 'from' to be less than 'to', but got from=0 >= to=-14214\nRun Again......2/2\nGive up!\nEpoch 0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 278/430 [02:03<01:07,  2.25it/s, v_num=n5aa]====================\nfetures_large.shape: torch.Size([1, 64, 410, 122])\nimage_ori_large.shape: torch.Size([1, 64, 410, 122])\npatch_size: 128\nLarge space: (282, -6)\nOri space: (282, -6)\nSampling space: (282, -6)\nRuntime Error random_ expects 'from' to be less than 'to', but got from=0 >= to=-1692\nRun Again......1/2\nHalfing image size\n====================\nfetures_large.shape: torch.Size([1, 64, 274, 82])\nimage_ori_large.shape: torch.Size([1, 64, 274, 82])\npatch_size: 128\nLarge space: (146, -46)\nOri space: (146, -46)\nSampling space: (146, -46)\nRuntime Error random_ expects 'from' to be less than 'to', but got from=0 >= to=-6716\nRun Again......2/2\nGive up!\nEpoch 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 377/430 [02:44<00:23,  2.29it/s, v_num=n5aa]====================\nfetures_large.shape: torch.Size([1, 64, 121, 450])\nimage_ori_large.shape: torch.Size([1, 64, 121, 450])\npatch_size: 128\nLarge space: (-7, 322)\nOri space: (-7, 322)\nSampling space: (-7, 322)\nRuntime Error random_ expects 'from' to be less than 'to', but got from=0 >= to=-2254\nRun Again......1/2\nHalfing image size\n====================\nfetures_large.shape: torch.Size([1, 64, 81, 300])\nimage_ori_large.shape: torch.Size([1, 64, 81, 300])\npatch_size: 128\nLarge space: (-47, 172)\nOri space: (-47, 172)\nSampling space: (-47, 172)\nRuntime Error random_ expects 'from' to be less than 'to', but got from=0 >= to=-8084\nRun Again......2/2\nGive up!\nEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 430/430 [03:07<00:00,  2.29it/s, v_num=n5aa]\nValidation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\nValidation:   0%|                                       | 0/108 [00:00<?, ?it/s]\u001b[A\nValidation DataLoader 0:   0%|                          | 0/108 [00:00<?, ?it/s]\u001b[A\nValidation DataLoader 0:   1%|‚ñè                 | 1/108 [00:00<00:12,  8.36it/s]\u001b[A\nValidation DataLoader 0:   2%|‚ñé                 | 2/108 [00:00<00:13,  7.71it/s]\u001b[A\nValidation DataLoader 0:   3%|‚ñå                 | 3/108 [00:00<00:15,  6.98it/s]\u001b[A\nValidation DataLoader 0:   4%|‚ñã                 | 4/108 [00:00<00:15,  6.61it/s]\u001b[A====================\nfetures_large.shape: torch.Size([1, 64, 123, 500])\nimage_ori_large.shape: torch.Size([1, 64, 123, 500])\npatch_size: 128\nLarge space: (-5, 372)\nOri space: (-5, 372)\nSampling space: (-5, 372)\nTraceback (most recent call last):\n  File \"/kaggle/working/local_learning_wsi/main.py\", line 178, in <module>\n    main(args)\n  File \"/kaggle/working/local_learning_wsi/main.py\", line 171, in main\n    trainer.fit(trainer_model, data_module)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 544, in fit\n    call._call_and_handle_interrupt(\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 44, in _call_and_handle_interrupt\n    return trainer_fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 580, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 989, in _run\n    results = self._run_stage()\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1035, in _run_stage\n    self.fit_loop.run()\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 202, in run\n    self.advance()\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 359, in advance\n    self.epoch_loop.run(self._data_fetcher)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 137, in run\n    self.on_advance_end(data_fetcher)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 285, in on_advance_end\n    self.val_loop.run()\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py\", line 182, in _decorator\n    return loop_run(self, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 134, in run\n    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 391, in _evaluation_step\n    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 309, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 403, in validation_step\n    return self.lightning_module.validation_step(*args, **kwargs)\n  File \"/kaggle/working/local_learning_wsi/trainer.py\", line 122, in validation_step\n    ft, y_k, loss_k = self(ft, label, ki=cur_k)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/kaggle/working/local_learning_wsi/trainer.py\", line 59, in forward\n    loss = self.loss_nets[ki](features, x=x, label=label)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/kaggle/working/local_learning_wsi/network/base_classifier.py\", line 50, in forward\n    ret = clf_loss(features, x, label)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/kaggle/working/local_learning_wsi/network/infopro_decoder.py\", line 79, in forward\n    sampling_pos = torch.randint(0, sampling_space[0] * sampling_space[1],\nRuntimeError: random_ expects 'from' to be less than 'to', but got from=0 >= to=-1860\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:                             epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: loss/part_1_step/dataloader_idx_0 ‚ñà‚ñÇ‚ñÇ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: loss/part_2_step/dataloader_idx_0 ‚ñà‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: loss/part_3_step/dataloader_idx_0 ‚ñà‚ñÅ‚ñÇ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: loss/part_4_step/dataloader_idx_0 ‚ñà‚ñÇ‚ñÇ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:                 loss/train_1_step ‚ñÑ‚ñÜ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñà‚ñÑ\n\u001b[34m\u001b[1mwandb\u001b[0m:                 loss/train_2_step ‚ñÇ‚ñÉ‚ñÜ‚ñÅ‚ñá‚ñà‚ñá‚ñÜ\n\u001b[34m\u001b[1mwandb\u001b[0m:                 loss/train_3_step ‚ñÑ‚ñÖ‚ñá‚ñÅ‚ñá‚ñà‚ñá‚ñÜ\n\u001b[34m\u001b[1mwandb\u001b[0m:                 loss/train_4_step ‚ñÉ‚ñÉ‚ñÜ‚ñÅ‚ñá‚ñà‚ñÖ‚ñÑ\n\u001b[34m\u001b[1mwandb\u001b[0m:        loss_step/dataloader_idx_0 ‚ñà‚ñÇ‚ñÇ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:                      lr-AdamW/pg1 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:                      lr-AdamW/pg2 ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m:               trainer/global_step ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                             epoch 0\n\u001b[34m\u001b[1mwandb\u001b[0m: loss/part_1_step/dataloader_idx_0 1.5127\n\u001b[34m\u001b[1mwandb\u001b[0m: loss/part_2_step/dataloader_idx_0 1.3418\n\u001b[34m\u001b[1mwandb\u001b[0m: loss/part_3_step/dataloader_idx_0 1.17188\n\u001b[34m\u001b[1mwandb\u001b[0m: loss/part_4_step/dataloader_idx_0 0.9165\n\u001b[34m\u001b[1mwandb\u001b[0m:                 loss/train_1_step 1.60938\n\u001b[34m\u001b[1mwandb\u001b[0m:                 loss/train_2_step 1.65527\n\u001b[34m\u001b[1mwandb\u001b[0m:                 loss/train_3_step 1.62305\n\u001b[34m\u001b[1mwandb\u001b[0m:                 loss/train_4_step 1.50781\n\u001b[34m\u001b[1mwandb\u001b[0m:        loss_step/dataloader_idx_0 0.9165\n\u001b[34m\u001b[1mwandb\u001b[0m:                      lr-AdamW/pg1 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m:                      lr-AdamW/pg2 2e-05\n\u001b[34m\u001b[1mwandb\u001b[0m:               trainer/global_step 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mtest_run\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/malev/UBC%20Ovarian%20Cancer%20Classification/runs/q7mgn5aa\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231125_115003-q7mgn5aa/logs\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"img = pyvips.Image.new_from_file(\"/kaggle/input/UBC-OCEAN/train_thumbnails/10077_thumbnail.png\")","metadata":{"execution":{"iopub.status.busy":"2023-11-25T11:47:03.774443Z","iopub.status.idle":"2023-11-25T11:47:03.774749Z","shell.execute_reply.started":"2023-11-25T11:47:03.774597Z","shell.execute_reply":"2023-11-25T11:47:03.774611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img.numpy().shape","metadata":{"execution":{"iopub.status.busy":"2023-11-25T10:14:29.290719Z","iopub.execute_input":"2023-11-25T10:14:29.291097Z","iopub.status.idle":"2023-11-25T10:14:29.318268Z","shell.execute_reply.started":"2023-11-25T10:14:29.291067Z","shell.execute_reply":"2023-11-25T10:14:29.317292Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(2210, 3000, 3)"},"metadata":{}}]},{"cell_type":"code","source":"import cv2\n\nimg = cv2.cvtColor(cv2.imread(\"/kaggle/input/UBC-OCEAN/train_thumbnails/10077_thumbnail.png\"), cv2.COLOR_BGR2RGB)\nimg.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-25T10:14:31.212881Z","iopub.execute_input":"2023-11-25T10:14:31.213239Z","iopub.status.idle":"2023-11-25T10:14:31.403187Z","shell.execute_reply.started":"2023-11-25T10:14:31.213211Z","shell.execute_reply":"2023-11-25T10:14:31.402254Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(2210, 3000, 3)"},"metadata":{}}]},{"cell_type":"code","source":"df.sort_values(by=[\"image_height\"])","metadata":{"execution":{"iopub.status.busy":"2023-11-25T11:55:13.953010Z","iopub.execute_input":"2023-11-25T11:55:13.953410Z","iopub.status.idle":"2023-11-25T11:55:13.968850Z","shell.execute_reply.started":"2023-11-25T11:55:13.953380Z","shell.execute_reply":"2023-11-25T11:55:13.967783Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"     image_id label  image_width  image_height  is_tma\n112     13568  LGSC         2964          2964    True\n37       4134    MC         2964          2964    True\n355     41586    CC         2964          2964    True\n423     50932  HGSC         2964          2964    True\n361     42857    CC         2964          2964    True\n..        ...   ...          ...           ...     ...\n226     28028  HGSC        48570         49215   False\n373     44530    MC        53239         49293   False\n256     30986    MC        66911         49395   False\n198     24563    CC        60533         49543   False\n334     39258  LGSC        47191         50155   False\n\n[538 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>label</th>\n      <th>image_width</th>\n      <th>image_height</th>\n      <th>is_tma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>112</th>\n      <td>13568</td>\n      <td>LGSC</td>\n      <td>2964</td>\n      <td>2964</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>4134</td>\n      <td>MC</td>\n      <td>2964</td>\n      <td>2964</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>355</th>\n      <td>41586</td>\n      <td>CC</td>\n      <td>2964</td>\n      <td>2964</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>423</th>\n      <td>50932</td>\n      <td>HGSC</td>\n      <td>2964</td>\n      <td>2964</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>361</th>\n      <td>42857</td>\n      <td>CC</td>\n      <td>2964</td>\n      <td>2964</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>226</th>\n      <td>28028</td>\n      <td>HGSC</td>\n      <td>48570</td>\n      <td>49215</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>373</th>\n      <td>44530</td>\n      <td>MC</td>\n      <td>53239</td>\n      <td>49293</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>256</th>\n      <td>30986</td>\n      <td>MC</td>\n      <td>66911</td>\n      <td>49395</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>24563</td>\n      <td>CC</td>\n      <td>60533</td>\n      <td>49543</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>334</th>\n      <td>39258</td>\n      <td>LGSC</td>\n      <td>47191</td>\n      <td>50155</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>538 rows √ó 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv(\"wsi_train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-25T10:29:06.968860Z","iopub.execute_input":"2023-11-25T10:29:06.969254Z","iopub.status.idle":"2023-11-25T10:29:06.978095Z","shell.execute_reply.started":"2023-11-25T10:29:06.969228Z","shell.execute_reply":"2023-11-25T10:29:06.977161Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df = df[df[\"type\"] == \"valid\"]","metadata":{"execution":{"iopub.status.busy":"2023-11-25T10:30:03.742885Z","iopub.execute_input":"2023-11-25T10:30:03.743242Z","iopub.status.idle":"2023-11-25T10:30:03.749309Z","shell.execute_reply.started":"2023-11-25T10:30:03.743213Z","shell.execute_reply":"2023-11-25T10:30:03.748183Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"df.iloc[3][\"slide_id\"]","metadata":{"execution":{"iopub.status.busy":"2023-11-25T10:30:46.045266Z","iopub.execute_input":"2023-11-25T10:30:46.046123Z","iopub.status.idle":"2023-11-25T10:30:46.052278Z","shell.execute_reply.started":"2023-11-25T10:30:46.046091Z","shell.execute_reply":"2023-11-25T10:30:46.051272Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/UBC-OCEAN/train_thumbnails/1660_thumbnail.png'"},"metadata":{}}]},{"cell_type":"code","source":"cv2.imread(\"/kaggle/input/UBC-OCEAN/train_thumbnails/1660_thumbnail.png\").shape","metadata":{"execution":{"iopub.status.busy":"2023-11-25T10:31:09.090075Z","iopub.execute_input":"2023-11-25T10:31:09.090433Z","iopub.status.idle":"2023-11-25T10:31:09.148995Z","shell.execute_reply.started":"2023-11-25T10:31:09.090395Z","shell.execute_reply":"2023-11-25T10:31:09.148073Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(736, 3000, 3)"},"metadata":{}}]},{"cell_type":"code","source":"for img_name in Path(\"/kaggle/input/UBC-OCEAN/train_thumbnails\").glob(\"*.png\"):\n    shape = pyvips.Image.new_from_file(img_name).numpy().shape\n    if any(np.array(shape[:2])) <= 128):\n        print(shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T12:00:41.272509Z","iopub.execute_input":"2023-11-25T12:00:41.272884Z","iopub.status.idle":"2023-11-25T12:00:41.592192Z","shell.execute_reply.started":"2023-11-25T12:00:41.272854Z","shell.execute_reply":"2023-11-25T12:00:41.591074Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_name \u001b[38;5;129;01min\u001b[39;00m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/UBC-OCEAN/train_thumbnails\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     shape \u001b[38;5;241m=\u001b[39m pyvips\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mnew_from_file(img_name)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(shape)\n","\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'tuple' and 'int'"],"ename":"TypeError","evalue":"'<=' not supported between instances of 'tuple' and 'int'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}